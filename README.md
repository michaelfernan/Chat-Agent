# ğŸš€ Chat Agent â€” FastAPI + Strands Agents + Ollama

Case tÃ©cnico: **API de Chat com Agente de IA**, capaz de:

- ğŸ’¬ conversar sobre conhecimento geral  
- â— usar uma **tool de cÃ¡lculo matemÃ¡tico** para resolver operaÃ§Ãµes automaticamente

Stack utilizada:

- **FastAPI** â†’ API HTTP  
- **Strands Agents** â†’ orquestraÃ§Ã£o do agente  
- **Ollama** â†’ LLM local  
- **strands-agents-tools** â†’ tool `calculator`  
- **Pytest** â†’ testes automatizados  
- **Docker / Docker Compose** â†’ empacotamento da API

---

## ğŸ§  VisÃ£o Geral da Arquitetura

1. O cliente envia `POST /chat` com `{ "message": "..." }`.
2. O FastAPI repassa a mensagem para o **Agent** carregado no startup (lifespan).
3. O Agent:
   - usa `OllamaModel` apontando para o host configurado (ex.: `http://host.docker.internal:11434` via Docker, ou `http://127.0.0.1:11434` local);
   - possui a tool `calculator` configurada.
4. O LLM decide:
   - responder diretamente, ou  
   - acionar a tool para cÃ¡lculos.
5. A API retorna um JSON no formato `{ "response": "..." }`.

---

## ğŸ—‚ Estrutura de Pastas

```bash
app/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ routes_chat.py          # endpoint POST /chat
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ builder.py              # construÃ§Ã£o do Agent
â”‚   â””â”€â”€ tools.py                # espaÃ§o para tools extras
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logging.py              # logging customizado
â”‚   â””â”€â”€ settings.py             # Pydantic Settings (.env)
â”œâ”€â”€ schemas/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ chat.py                 # ChatRequest / ChatResponse
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ runtime.py              # lifespan + get_agent
â””â”€â”€ main.py                     # App FastAPI + /health
```

---

## âš™ï¸ ConfiguraÃ§Ã£o do Ambiente (.env)

Crie um arquivo `.env` na raiz do projeto:

```env
# Host/modelo usados pela aplicaÃ§Ã£o (modo local)
CHAT_OLLAMA_HOST=http://127.0.0.1:11434
CHAT_OLLAMA_MODEL_ID=llama3.1

# ConfiguraÃ§Ã£o de host/porta da API
CHAT_API_HOST=0.0.0.0
CHAT_API_PORT=8000
```

> No ambiente Docker, essas variÃ¡veis podem ser sobrescritas no `docker-compose.yml`.

---

## ğŸ Rodando Localmente (sem Docker)

### 1ï¸âƒ£ Criar e ativar ambiente virtual

```bash
python3 -m venv .venv
source .venv/bin/activate
```

### 2ï¸âƒ£ Instalar dependÃªncias

```bash
pip install .
```

ou, alternativamente:

```bash
pip install fastapi uvicorn[standard] pydantic pydantic-settings strands-agents strands-agents-tools ollama pytest
```

---

## ğŸ¤– Configurar e rodar o Ollama (host)

### InstalaÃ§Ã£o

Siga a documentaÃ§Ã£o oficial do Ollama para Linux. Exemplo (padrÃ£o script oficial):

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

> **Obs.:** em ambiente Docker Ã© importante que o servidor Ollama esteja rodando **no host** escutando em `127.0.0.1:11434`, pois o container vai acessÃ¡â€‘lo via `host.docker.internal`.

### Baixar o modelo

> âš ï¸ Importante:  
> `llama3` **nÃ£o suporta tools**.  
> Use **`llama3.1`**.

```bash
ollama pull llama3.1
```

### Subir o servidor Ollama

```bash
ollama serve
```

(Ou habilite o serviÃ§o para subir automaticamente, conforme a instalaÃ§Ã£o.)

---

## ğŸƒâ€â™‚ï¸ Rodando o Servidor FastAPI (local)

Com o ambiente virtual ativo e o Ollama rodando:

```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

Endpoints principais:

- Health Check â†’ `GET /health`
- Chat â†’ `POST /chat`

---

## ğŸ³ Rodando com Docker / Docker Compose

O projeto inclui suporte a Docker. Exemplo de `docker-compose.yml`:

```yaml
services:
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: chat-agent-api
    restart: unless-stopped
    environment:
      # Ollama rodando no HOST, acessÃ­vel via host.docker.internal
      OLLAMA_HOST: "http://host.docker.internal:11434"
      CHAT_OLLAMA_HOST: "http://host.docker.internal:11434"
      CHAT_OLLAMA_MODEL_ID: "llama3.1"

      CHAT_API_HOST: "0.0.0.0"
      CHAT_API_PORT: "8000"
    ports:
      - "8000:8000"
    volumes:
      - .:/app
    extra_hosts:
      - "host.docker.internal:host-gateway"
```

> Certifiqueâ€‘se de que o **Ollama estÃ¡ rodando no host** antes de subir o container.

### Subir a API via Docker

```bash
docker compose up --build
```

Verificar:

```bash
curl http://localhost:8000/health
# {"status":"ok"}
```

---

## ğŸ” Endpoints

### `GET /health`

```json
{ "status": "ok" }
```

---

### `POST /chat`

#### Exemplo de requisiÃ§Ã£o:

```json
{
  "message": "Qual a raiz quadrada de 144?"
}
```

#### Exemplo de resposta:

```json
{
  "response": "A raiz quadrada de 144 Ã© 12.
"
}
```

Nos logs do servidor Ã© possÃ­vel ver o uso da tool:

```text
Tool #2: calculator
```

---

## ğŸ§ª Testes Automatizados

Os testes utilizam **pytest** e o `TestClient` do FastAPI.

Arquivo principal de teste:

```text
app/tests/test_chat_endpoint.py
```

### PrÃ©â€‘requisitos para os testes passarem

1. DependÃªncias instaladas no ambiente virtual:
   ```bash
   pip install .
   pip install pytest
   ```
2. Servidor do **Ollama rodando** e acessÃ­vel no host configurado em `CHAT_OLLAMA_HOST`
   (por padrÃ£o, `http://127.0.0.1:11434` via `.env`).

### Rodando todos os testes

```bash
pytest -q
```

### Rodando apenas o teste do endpoint de chat

```bash
pytest app/tests/test_chat_endpoint.py -q
```

---

## ğŸ§ª Testes manuais via `curl`

### â— 1. MultiplicaÃ§Ã£o

```bash
curl -X POST http://localhost:8000/chat   -H "Content-Type: application/json"   -d '{"message": "Quanto Ã© 1234 * 5678?"}'
```

Resposta esperada (exemplo):

```json
{
  "response": "O resultado da multiplicaÃ§Ã£o de 1234 por 5678 Ã© 7.006.652.
"
}
```

---

### âˆš 2. Raiz quadrada

```bash
curl -X POST http://localhost:8000/chat   -H "Content-Type: application/json"   -d '{"message": "Qual a raiz quadrada de 144?"}'
```

Resposta esperada (exemplo):

```json
{
  "response": "A raiz quadrada de 144 Ã© 12.
"
}
```

---

## ğŸ“¦ Versionamento e Boas PrÃ¡ticas

O repositÃ³rio inclui:

- `.gitignore` ignorando:
  - `.venv/`
  - `.env`
  - `__pycache__/`
- `pyproject.toml` com dependÃªncias e ferramentas de desenvolvimento
- Arquitetura modular seguindo boas prÃ¡ticas

---

## ğŸ§  ObservaÃ§Ãµes TÃ©cnicas

- O agente Ã© carregado **uma Ãºnica vez** no startup via `lifespan` (arquivo `utils/runtime.py`).
- Logging estruturado com `request_id` por requisiÃ§Ã£o.
- Suporte nativo para novas tools (basta adicionar em `agent/tools.py`).
- Zero dependÃªncias em serviÃ§os externos de nuvem: o LLM roda 100% local via Ollama.
